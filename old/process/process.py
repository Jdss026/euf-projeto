import os
import torch
from dotenv import load_dotenv

# Importa√ß√µes da API Marker
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import save_output

load_dotenv()

def main():
    fpath = "./scrap/input/euf_2023_2.pdf"
    output_dir = "./process/output"

    # --- CONFIGURA√á√ïES DE EMERG√äNCIA PARA 4GB ---
    # 1. For√ßa o modelo de Layout para a CPU (Libera ~1.5GB de VRAM)
    os.environ["LAYOUT_DEVICE"] = "cpu"
    
    # 2. Mant√©m o OCR e Equa√ß√µes na GPU
    os.environ["TORCH_DEVICE"] = "cuda"
    
    # 3. Reduz resolu√ß√µes para economizar mem√≥ria de processamento
    os.environ["RECOGNITION_BATCH_SIZE"] = "1"
    os.environ["LAYOUT_BATCH_SIZE"] = "1"
    
    # 4. Define apenas a primeira p√°gina para o teste
    os.environ["PAGE_RANGE"] = "0"

    print("üöÄ Carregando modelos com Offloading (Layout -> CPU | OCR -> GPU)...")
    
    # create_model_dict ler√° as vari√°veis de ambiente acima
    model_dict = create_model_dict() 

    # Inicializa o conversor
    converter = PdfConverter(artifact_dict=model_dict)

    print(f"üìÑ Processando p√°gina 1 de: {fpath}...")
    
    try:
        # Executa a convers√£o
        rendered = converter(fpath) 
        
        # Salva o resultado
        save_output(rendered, output_dir, "teste_euf_p1")
        print(f"‚úÖ Sucesso! Arquivos em: {output_dir}")
        
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print("‚ùå Erro: VRAM insuficiente mesmo com Offloading. Tente fechar o navegador.")
        else:
            print(f"‚ùå Erro inesperado: {e}")
            
    finally:
        # Limpeza agressiva de mem√≥ria
        if 'model_dict' in locals(): del model_dict
        if 'converter' in locals(): del converter
        torch.cuda.empty_cache()

if __name__ == "__main__":
    main()